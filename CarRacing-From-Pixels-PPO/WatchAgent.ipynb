{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch a Smart Agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Start the Environment for Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/envs/rl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "import os\n",
    "import time\n",
    "\n",
    "from agent import Agent\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def rgb2gray(rgb, norm=True):\n",
    "        # rgb image -> gray [0, 1]\n",
    "    gray = np.dot(rgb[..., :], [0.299, 0.587, 0.114])\n",
    "    if norm:\n",
    "        # normalize\n",
    "        gray = gray / 128. - 1.\n",
    "    return gray\n",
    "\n",
    "seed = 0\n",
    "img_stack = 4\n",
    "action_repeat = 10\n",
    "env = gym.make('CarRacing-v0', verbose=0)\n",
    "state = env.reset()\n",
    "reward_threshold = env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper():\n",
    "    \"\"\"\n",
    "    Environment wrapper for CarRacing \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env  \n",
    "\n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "        self.av_r = self.reward_memory()\n",
    "\n",
    "        self.die = False\n",
    "        img_rgb = env.reset()\n",
    "        img_gray = rgb2gray(img_rgb)\n",
    "        self.stack = [img_gray] * img_stack  # four frames for decision\n",
    "        return np.array(self.stack)\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        for i in range(action_repeat):\n",
    "            img_rgb, reward, die, _ = env.step(action)\n",
    "            # don't penalize \"die state\"\n",
    "            if die:\n",
    "                reward += 100\n",
    "            # green penalty\n",
    "            if np.mean(img_rgb[:, :, 1]) > 185.0:\n",
    "                reward -= 0.05\n",
    "            total_reward += reward\n",
    "            # if no reward recently, end the episode\n",
    "            done = True if self.av_r(reward) <= -0.1 else False\n",
    "            if done or die:\n",
    "                break\n",
    "        img_gray = rgb2gray(img_rgb)\n",
    "        self.stack.pop(0)\n",
    "        self.stack.append(img_gray)\n",
    "        assert len(self.stack) == img_stack\n",
    "        return np.array(self.stack), total_reward, done, die\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def reward_memory():\n",
    "        # record reward for last 100 steps\n",
    "        count = 0\n",
    "        length = 100\n",
    "        history = np.zeros(length)\n",
    "\n",
    "        def memory(reward):\n",
    "            nonlocal count\n",
    "            history[count] = reward\n",
    "            count = (count + 1) % length\n",
    "            return np.mean(history)\n",
    "\n",
    "        return memory\n",
    "    \n",
    "agent = Agent(device)\n",
    "\n",
    "env_wrap = Wrapper(env)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(agent, directory, filename):\n",
    "    agent.net.load_state_dict(torch.load(os.path.join(directory,filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prepare Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import os\n",
    "\n",
    "def play(env, agent, n_episodes):\n",
    "    state = env_wrap.reset()\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env_wrap.reset()        \n",
    "        score = 0\n",
    "        \n",
    "        time_start = time.time()\n",
    "        \n",
    "        while True:\n",
    "            action, a_logp = agent.select_action(state)\n",
    "            env.render()\n",
    "            next_state, reward, done, die = env_wrap.step( \\\n",
    "                action * np.array([2., 1., 1.]) + np.array([-1., 0., 0.]))\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            if done or die:\n",
    "                break \n",
    "\n",
    "        s = (int)(time.time() - time_start)\n",
    "        \n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "\n",
    "        print('Episode {}\\tAverage Score: {:.2f},\\tScore: {:.2f} \\tTime: {:02}:{:02}:{:02}'\\\n",
    "                  .format(i_episode, np.mean(scores_deque), score, s//3600, s%3600//60, s%60))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load and Play: Score = 350-550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 664.18,\tScore: 664.18 \tTime: 00:00:10\n",
      "Episode 2\tAverage Score: 441.62,\tScore: 219.07 \tTime: 00:00:06\n",
      "Episode 3\tAverage Score: 497.46,\tScore: 609.12 \tTime: 00:00:09\n",
      "Episode 4\tAverage Score: 523.27,\tScore: 600.73 \tTime: 00:00:09\n",
      "Episode 5\tAverage Score: 538.48,\tScore: 599.30 \tTime: 00:00:09\n"
     ]
    }
   ],
   "source": [
    "load(agent, 'dir_chk', 'model_weights_350-550.pth')\n",
    "play(env, agent, n_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load and Play: Score = 580-660"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 603.72,\tScore: 603.72 \tTime: 00:00:12\n",
      "Episode 2\tAverage Score: 593.94,\tScore: 584.16 \tTime: 00:00:11\n",
      "Episode 3\tAverage Score: 432.31,\tScore: 109.06 \tTime: 00:00:08\n",
      "Episode 4\tAverage Score: 480.99,\tScore: 627.01 \tTime: 00:00:11\n",
      "Episode 5\tAverage Score: 517.67,\tScore: 664.38 \tTime: 00:00:11\n"
     ]
    }
   ],
   "source": [
    "load(agent, 'dir_chk', 'model_weights_480-660.pth')\n",
    "play(env, agent, n_episodes=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Load and Play: Score = 820-980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 858.52,\tScore: 858.52 \tTime: 00:00:09\n",
      "Episode 2\tAverage Score: 890.43,\tScore: 922.33 \tTime: 00:00:09\n",
      "Episode 3\tAverage Score: 927.22,\tScore: 1000.80 \tTime: 00:00:09\n",
      "Episode 4\tAverage Score: 889.14,\tScore: 774.92 \tTime: 00:00:10\n",
      "Episode 5\tAverage Score: 891.12,\tScore: 899.02 \tTime: 00:00:09\n"
     ]
    }
   ],
   "source": [
    "load(agent, 'dir_chk', 'model_weights_820-980.pth')\n",
    "play(env, agent, n_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 1003.20,\tScore: 1003.20 \tTime: 00:00:13\n",
      "Episode 2\tAverage Score: 999.96,\tScore: 996.71 \tTime: 00:00:10\n"
     ]
    }
   ],
   "source": [
    "load(agent, 'dir_chk', 'model_weights_final2.pth')\n",
    "play(env, agent, n_episodes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, agent.net.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "445955"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (cnn_base): Sequential(\n",
       "    (0): Conv2d(4, 8, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (11): ReLU()\n",
       "  )\n",
       "  (v): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (alpha_head): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=3, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       "  (beta_head): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=3, bias=True)\n",
       "    (1): Softplus(beta=1, threshold=20)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.net.to(torch.device(\"cpu\"),dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 47, 47]             520\n",
      "              ReLU-2            [-1, 8, 47, 47]               0\n",
      "            Conv2d-3           [-1, 16, 23, 23]           1,168\n",
      "              ReLU-4           [-1, 16, 23, 23]               0\n",
      "            Conv2d-5           [-1, 32, 11, 11]           4,640\n",
      "              ReLU-6           [-1, 32, 11, 11]               0\n",
      "            Conv2d-7             [-1, 64, 5, 5]          18,496\n",
      "              ReLU-8             [-1, 64, 5, 5]               0\n",
      "            Conv2d-9            [-1, 128, 3, 3]          73,856\n",
      "             ReLU-10            [-1, 128, 3, 3]               0\n",
      "           Conv2d-11            [-1, 256, 1, 1]         295,168\n",
      "             ReLU-12            [-1, 256, 1, 1]               0\n",
      "           Linear-13                  [-1, 100]          25,700\n",
      "             ReLU-14                  [-1, 100]               0\n",
      "           Linear-15                    [-1, 1]             101\n",
      "           Linear-16                  [-1, 100]          25,700\n",
      "             ReLU-17                  [-1, 100]               0\n",
      "           Linear-18                    [-1, 3]             303\n",
      "         Softplus-19                    [-1, 3]               0\n",
      "           Linear-20                    [-1, 3]             303\n",
      "         Softplus-21                    [-1, 3]               0\n",
      "================================================================\n",
      "Total params: 445,955\n",
      "Trainable params: 445,955\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.14\n",
      "Forward/backward pass size (MB): 0.51\n",
      "Params size (MB): 1.70\n",
      "Estimated Total Size (MB): 2.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(agent.net, (4,96,96),device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
