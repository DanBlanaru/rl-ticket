{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/envs/rl/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_steps:  1600\n",
      "device:  cuda:0\n",
      "state dim:  24\n",
      "action dim:  Box(4,)\n",
      "leraning rate:  8e-05\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from sac_agent import soft_actor_critic_agent\n",
    "from replay_memory import ReplayMemory\n",
    "\n",
    "seed=0  \n",
    "env = gym.make('BipedalWalker-v3')\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "env.seed(seed)\n",
    "max_steps = env._max_episode_steps\n",
    "print('max_steps: ', max_steps)\n",
    "\n",
    "batch_size=256\n",
    "\n",
    "LEARNING_RATE=0.00008 # lr = 0.0001 for BipedalWalker-SAC_lr0001\n",
    "eval=True  ## \n",
    "start_steps=10000 ## Steps sampling random actions\n",
    "replay_size=1000000 ## size of replay buffer\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# alpha=0.2  # relative importance of the entropy\n",
    "# gamma=0.99  # discount factor \n",
    "# tau=0.005  # target smoothing coefficient(Ï„)\n",
    "\n",
    "agent = soft_actor_critic_agent(env.observation_space.shape[0], env.action_space, \\\n",
    "        device=device, hidden_size=256, lr=LEARNING_RATE, gamma=0.99, tau=0.005, alpha=0.2)\n",
    "\n",
    "memory = ReplayMemory(replay_size)\n",
    "\n",
    "print('device: ', device)\n",
    "print('state dim: ', env.observation_space.shape[0])\n",
    "print('action dim: ', env.action_space)\n",
    "print('leraning rate: ', LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(agent, directory, filename, suffix):\n",
    "    torch.save(agent.policy.state_dict(), '%s/%s_actor_%s.pth' % (directory, filename, suffix))\n",
    "    torch.save(agent.critic.state_dict(), '%s/%s_critic_%s.pth' % (directory, filename, suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep.: 0, Total Steps: 10, Ep.Steps: 10, Score: -1.44, Avg.Score: -1.44, Time: 00:00:00\n",
      "Ep.: 1, Total Steps: 20, Ep.Steps: 10, Score: -1.14, Avg.Score: -1.29, Time: 00:00:00\n",
      "Ep.: 2, Total Steps: 30, Ep.Steps: 10, Score: -0.80, Avg.Score: -1.12, Time: 00:00:00\n",
      "Ep.: 3, Total Steps: 40, Ep.Steps: 10, Score: -0.52, Avg.Score: -0.97, Time: 00:00:00\n",
      "Ep.: 4, Total Steps: 50, Ep.Steps: 10, Score: -0.74, Avg.Score: -0.93, Time: 00:00:00\n",
      "Ep.: 5, Total Steps: 60, Ep.Steps: 10, Score: -0.46, Avg.Score: -0.85, Time: 00:00:00\n",
      "Ep.: 6, Total Steps: 70, Ep.Steps: 10, Score: -2.15, Avg.Score: -1.04, Time: 00:00:00\n",
      "Ep.: 7, Total Steps: 80, Ep.Steps: 10, Score: -0.79, Avg.Score: -1.00, Time: 00:00:00\n",
      "Ep.: 8, Total Steps: 90, Ep.Steps: 10, Score: -0.92, Avg.Score: -1.00, Time: 00:00:00\n",
      "Ep.: 9, Total Steps: 100, Ep.Steps: 10, Score: -0.86, Avg.Score: -0.98, Time: 00:00:00\n",
      "Ep.: 10, Total Steps: 110, Ep.Steps: 10, Score: -0.90, Avg.Score: -0.97, Time: 00:00:00\n",
      "Ep.: 11, Total Steps: 120, Ep.Steps: 10, Score: -0.99, Avg.Score: -0.98, Time: 00:00:00\n",
      "Ep.: 12, Total Steps: 130, Ep.Steps: 10, Score: -0.75, Avg.Score: -0.96, Time: 00:00:00\n",
      "Ep.: 13, Total Steps: 140, Ep.Steps: 10, Score: -1.46, Avg.Score: -0.99, Time: 00:00:00\n",
      "Ep.: 14, Total Steps: 150, Ep.Steps: 10, Score: -0.87, Avg.Score: -0.99, Time: 00:00:00\n",
      "Ep.: 15, Total Steps: 160, Ep.Steps: 10, Score: -1.47, Avg.Score: -1.02, Time: 00:00:00\n",
      "Ep.: 16, Total Steps: 170, Ep.Steps: 10, Score: -0.80, Avg.Score: -1.00, Time: 00:00:00\n",
      "Ep.: 17, Total Steps: 180, Ep.Steps: 10, Score: -1.62, Avg.Score: -1.04, Time: 00:00:00\n",
      "Ep.: 18, Total Steps: 190, Ep.Steps: 10, Score: -1.15, Avg.Score: -1.04, Time: 00:00:00\n",
      "Ep.: 19, Total Steps: 200, Ep.Steps: 10, Score: -0.80, Avg.Score: -1.03, Time: 00:00:00\n",
      "Ep.: 20, Total Steps: 210, Ep.Steps: 10, Score: -0.96, Avg.Score: -1.03, Time: 00:00:00\n",
      "Ep.: 21, Total Steps: 220, Ep.Steps: 10, Score: -0.78, Avg.Score: -1.02, Time: 00:00:00\n",
      "Ep.: 22, Total Steps: 230, Ep.Steps: 10, Score: -0.90, Avg.Score: -1.01, Time: 00:00:00\n",
      "Ep.: 23, Total Steps: 240, Ep.Steps: 10, Score: -0.78, Avg.Score: -1.00, Time: 00:00:00\n",
      "Ep.: 24, Total Steps: 250, Ep.Steps: 10, Score: -0.54, Avg.Score: -0.98, Time: 00:00:00\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [256, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0b4d268ba7ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscores_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_scores_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msac_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-0b4d268ba7ac>\u001b[0m in \u001b[0;36msac_train\u001b[0;34m(max_steps)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m# Update parameters of all the networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mupdates\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/licenta/main/rl-ticket/BipedalWalker-Soft-Actor-Critic/sac_agent.py\u001b[0m in \u001b[0;36mupdate_parameters\u001b[0;34m(self, memory, batch_size, updates)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rl/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [256, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "import time\n",
    "from  collections  import deque\n",
    "\n",
    "def sac_train(max_steps):\n",
    "\n",
    "    total_numsteps = 0\n",
    "    updates = 0\n",
    "    num_episodes = 10001\n",
    "    updates=0\n",
    "\n",
    "    time_start = time.time()\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores_array = []\n",
    "    avg_scores_array = [] \n",
    "    \n",
    "    for i_episode in range(num_episodes): \n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "\n",
    "        for step in range(max_steps):    \n",
    "            if start_steps > total_numsteps:\n",
    "                action = env.action_space.sample()  # Sample random action\n",
    "            else:\n",
    "                action = agent.select_action(state)  # Sample action from policy\n",
    "\n",
    "            if len(memory) > batch_size:\n",
    "                \n",
    "                # Update parameters of all the networks\n",
    "                agent.update_parameters(memory, batch_size, updates)\n",
    "\n",
    "                updates += 1\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action) # Step\n",
    "            episode_steps += 1\n",
    "            total_numsteps += 1\n",
    "            episode_reward += reward\n",
    "\n",
    "            mask = 1 if episode_steps == env._max_episode_steps else float(not done)\n",
    "\n",
    "            memory.push(state, action, reward, next_state, mask) # Append transition to memory\n",
    "\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores_deque.append(episode_reward)\n",
    "        scores_array.append(episode_reward)        \n",
    "        avg_score = np.mean(scores_deque)\n",
    "        avg_scores_array.append(avg_score)\n",
    "        \n",
    "#         if i_episode % 20 == 0 and i_episode > 0:\n",
    "#             save(agent, 'dir_chk_lr00008', 'weights', str(i_episode))\n",
    "\n",
    "        s =  (int)(time.time() - time_start)\n",
    "            \n",
    "        print(\"Ep.: {}, Total Steps: {}, Ep.Steps: {}, Score: {:.2f}, Avg.Score: {:.2f}, Time: {:02}:{:02}:{:02}\".\\\n",
    "            format(i_episode, total_numsteps, episode_steps, episode_reward, avg_score, \\\n",
    "                  s//3600, s%3600//60, s%60))\n",
    "\n",
    "                    \n",
    "        if (avg_score > 300.5):\n",
    "            print('Solved environment with Avg Score:  ', avg_score)\n",
    "            break;\n",
    "            \n",
    "    return scores_array, avg_scores_array \n",
    "\n",
    "scores, avg_scores = sac_train(max_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(agent, 'dir_chk_lr00008', 'weights', 'final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print('length of scores: ', len(scores), ', len of avg_scores: ', len(avg_scores))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores, label=\"Score\")\n",
    "plt.plot(np.arange(1, len(avg_scores)+1), avg_scores, label=\"Avg on 100 episodes\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1)) \n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episodes #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, agent, num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    \n",
    "    for i_episode in range(num_episodes + 1):\n",
    "        \n",
    "        state = env.reset()\n",
    "        score = 0                    \n",
    "        time_start = time.time()\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            action = agent.select_action(state, eval=True)\n",
    "            env.render()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            score += reward \n",
    "            state = next_state\n",
    "    \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        s = (int)(time.time() - time_start)\n",
    "        \n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)    \n",
    "        \n",
    "        print('Episode {}\\tAverage Score: {:.2f},\\tScore: {:.2f} \\tTime: {:02}:{:02}:{:02}'\\\n",
    "                  .format(i_episode, np.mean(scores_deque), score, s//3600, s%3600//60, s%60)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(env=env, agent=agent, num_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(env=env, agent=agent, num_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
